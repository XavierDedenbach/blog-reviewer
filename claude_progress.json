{
  "timestamp": 1756949554.298757,
  "pr_title": "Feature/pr 005 review workflow orchestrator",
  "response": "## REQUIREMENT_ANALYSIS\n\nLet me break down each requirement systematically:\n\n### Core Implementation Requirements:\n1. **Create workflow orchestration engine**: Need a central orchestrator to manage multi-step review workflows with parallel task execution\n2. **Implement review state management**: Track state transitions (pending \u2192 in_progress \u2192 completed/failed) with persistence and consistency\n3. **Add task scheduling and queuing**: Priority-based task queue with async processing and dependencies\n4. **Create workflow configuration system**: Define workflow templates with configurable steps, timeouts, and retry policies\n5. **Implement priority-based processing**: High/Medium/Low priority queue system with preemption capabilities\n6. **Add workflow monitoring and logging**: Real-time progress tracking with structured logging and metrics\n7. **Create error handling and recovery**: Graceful failure handling with rollback and recovery mechanisms\n8. **Implement workflow retry mechanisms**: Exponential backoff, max retry limits, and selective retry policies\n9. **Add performance metrics collection**: Timing, throughput, and resource usage metrics\n10. **Create workflow visualization tools**: Status reporting and progress visualization\n\n### Testing Requirements:\n1. **Test workflow state transitions**: Verify all valid transitions and reject invalid ones\n2. **Test task scheduling accuracy**: Ensure correct ordering and timing\n3. **Test priority-based processing**: Verify high-priority tasks execute first\n4. **Test error handling and recovery**: Simulate failures and verify recovery\n5. **Test workflow retry mechanisms**: Test exponential backoff and retry limits\n6. **Test performance under load**: Concurrent workflow execution stress testing\n\n### Dependencies & Challenges:\n- Integration with existing database models (Article, Review)\n- Async/await pattern consistency with existing codebase\n- State persistence in MongoDB with atomic updates\n- Error propagation through workflow layers\n- Resource management for concurrent workflows\n- Monitoring integration without performance impact\n\n## SOLUTION_ARCHITECTURE\n\n### Design Philosophy:\nFollowing existing patterns in the codebase:\n- Use Pydantic models for data validation (like Article, Author models)\n- Async operations with motor/MongoDB (like existing database operations)\n- Service layer pattern with clear separation of concerns\n- Factory pattern for test data generation\n\n### Core Components:\n\n1. **WorkflowOrchestrator**: Central engine managing workflow lifecycle\n2. **WorkflowState**: Pydantic model tracking workflow status and progress\n3. **TaskQueue**: Priority-based async task scheduler\n4. **WorkflowConfig**: Configuration templates for different workflow types\n5. **WorkflowMonitor**: Metrics collection and progress tracking\n6. **ErrorHandler**: Recovery and retry logic\n\n### Architecture Decision Rationale:\n- **State Machine Pattern**: Ensures valid transitions and prevents inconsistent states\n- **Observer Pattern**: For monitoring without tight coupling\n- **Strategy Pattern**: For different retry and error handling policies\n- **Async Queue**: Non-blocking task execution with priority handling\n\n### Integration Points:\n- Uses existing database connection patterns\n- Integrates with Review model for blog review workflows\n- Follows existing logging patterns\n- Uses same error handling conventions\n\n## IMPLEMENTATION_STEPS\n\n1. **Create Core Models** (Foundation)\n   - WorkflowState, WorkflowConfig, TaskDefinition models\n   - State transition validation\n\n2. **Implement Task Queue System** (Scheduling Layer)\n   - Priority-based async queue\n   - Task dependency management\n\n3. **Build Workflow Orchestrator** (Core Engine)\n   - State management\n   - Task scheduling integration\n   - Error handling coordination\n\n4. **Add Monitoring & Metrics** (Observability)\n   - Performance metrics collection\n   - Progress tracking\n   - Logging integration\n\n5. **Implement Error Handling** (Reliability)\n   - Retry mechanisms\n   - Recovery procedures\n   - Rollback capabilities\n\n6. **Create Configuration System** (Flexibility)\n   - Workflow templates\n   - Environment-specific settings\n\n7. **Add Visualization Tools** (Usability)\n   - Status reporting\n   - Progress visualization\n\n## FILES_TO_CREATE_OR_MODIFY\n\n### New Files:\n- `core/review_orchestrator/__init__.py` - Package initialization\n- `core/review_orchestrator/models.py` - Workflow data models\n- `core/review_orchestrator/orchestrator.py` - Main orchestration engine\n- `core/review_orchestrator/task_queue.py` - Priority-based task scheduling\n- `core/review_orchestrator/state_manager.py` - State management\n- `core/review_orchestrator/config.py` - Workflow configuration\n- `core/review_orchestrator/monitor.py` - Monitoring and metrics\n- `core/review_orchestrator/error_handler.py` - Error handling and recovery\n- `core/review_orchestrator/visualization.py` - Workflow visualization tools\n\n### Test Files:\n- `tests/unit/review_orchestrator/` - Unit tests for all components\n- `tests/integration/review_orchestrator/` - Integration tests\n- `tests/performance/test_workflow_load.py` - Performance tests\n\n### Modified Files:\n- `core/__init__.py` - Add orchestrator imports\n- `requirements.txt` - Add any new dependencies\n\n## TEST_FILES\n\n```python\n# File: tests/unit/review_orchestrator/__init__.py\n\"\"\"Unit tests for review orchestrator module.\"\"\"\n```\n\n```python\n# File: tests/unit/review_orchestrator/test_models.py\n\"\"\"\nUnit tests for workflow orchestrator models.\n\"\"\"\n\nimport pytest\nfrom datetime import datetime, timedelta\nfrom bson import ObjectId\nfrom pydantic import ValidationError\n\nfrom core.review_orchestrator.models import (\n    WorkflowState, TaskDefinition, TaskStatus, WorkflowStatus,\n    WorkflowConfig, TaskPriority, RetryPolicy, WorkflowMetrics\n)\n\n\nclass TestWorkflowState:\n    \"\"\"Test WorkflowState model.\"\"\"\n    \n    def test_workflow_state_creation(self):\n        \"\"\"Test creating a workflow state.\"\"\"\n        workflow_id = ObjectId()\n        review_id = ObjectId()\n        \n        state = WorkflowState(\n            workflow_id=workflow_id,\n            review_id=review_id,\n            status=WorkflowStatus.PENDING,\n            current_step=\"content_analysis\",\n            total_steps=4\n        )\n        \n        assert state.workflow_id == workflow_id\n        assert state.review_id == review_id\n        assert state.status == WorkflowStatus.PENDING\n        assert state.current_step == \"content_analysis\"\n        assert state.total_steps == 4\n        assert state.progress == 0.0\n        assert state.retry_count == 0\n        assert isinstance(state.created_at, datetime)\n        assert state.updated_at == state.created_at\n    \n    def test_workflow_state_progress_calculation(self):\n        \"\"\"Test progress calculation.\"\"\"\n        state = WorkflowState(\n            workflow_id=ObjectId(),\n            review_id=ObjectId(),\n            status=WorkflowStatus.IN_PROGRESS,\n            current_step=\"style_analysis\",\n            total_steps=4,\n            completed_steps=2\n        )\n        \n        assert state.progress == 0.5\n    \n    def test_workflow_state_invalid_progress(self):\n        \"\"\"Test invalid progress values.\"\"\"\n        with pytest.raises(ValidationError):\n            WorkflowState(\n                workflow_id=ObjectId(),\n                review_id=ObjectId(),\n                status=WorkflowStatus.IN_PROGRESS,\n                current_step=\"test\",\n                total_steps=4,\n                completed_steps=5  # More completed than total\n            )\n    \n    def test_workflow_state_status_transitions(self):\n        \"\"\"Test valid status transitions.\"\"\"\n        state = WorkflowState(\n            workflow_id=ObjectId(),\n            review_id=ObjectId(),\n            status=WorkflowStatus.PENDING,\n            current_step=\"init\",\n            total_steps=3\n        )\n        \n        # Valid transitions\n        valid_transitions = [\n            (WorkflowStatus.PENDING, WorkflowStatus.IN_PROGRESS),\n            (WorkflowStatus.IN_PROGRESS, WorkflowStatus.COMPLETED),\n            (WorkflowStatus.IN_PROGRESS, WorkflowStatus.FAILED),\n            (WorkflowStatus.FAILED, WorkflowStatus.RETRYING),\n            (WorkflowStatus.RETRYING, WorkflowStatus.IN_PROGRESS),\n        ]\n        \n        for from_status, to_status in valid_transitions:\n            state.status = from_status\n            assert state.can_transition_to(to_status)\n\n\nclass TestTaskDefinition:\n    \"\"\"Test TaskDefinition model.\"\"\"\n    \n    def test_task_definition_creation(self):\n        \"\"\"Test creating a task definition.\"\"\"\n        task = TaskDefinition(\n            name=\"content_analysis\",\n            description=\"Analyze content structure and quality\",\n            priority=TaskPriority.HIGH,\n            timeout_seconds=300,\n            retry_policy=RetryPolicy(max_retries=3, backoff_multiplier=2.0)\n        )\n        \n        assert task.name == \"content_analysis\"\n        assert task.priority == TaskPriority.HIGH\n        assert task.timeout_seconds == 300\n        assert task.retry_policy.max_retries == 3\n        assert task.retry_policy.backoff_multiplier == 2.0\n    \n    def test_task_definition_defaults(self):\n        \"\"\"Test task definition default values.\"\"\"\n        task = TaskDefinition(\n            name=\"test_task\",\n            description=\"Test task\"\n        )\n        \n        assert task.priority == TaskPriority.MEDIUM\n        assert task.timeout_seconds == 60\n        assert task.dependencies == []\n        assert task.retry_policy.max_retries == 3\n        assert task.retry_policy.base_delay_seconds == 1\n        assert task.retry_policy.backoff_multiplier == 2.0\n    \n    def test_task_priority_ordering(self):\n        \"\"\"Test task priority ordering.\"\"\"\n        high_task = TaskDefinition(name=\"high\", description=\"High priority\", priority=TaskPriority.HIGH)\n        medium_task = TaskDefinition(name=\"medium\", description=\"Medium priority\", priority=TaskPriority.MEDIUM)\n        low_task = TaskDefinition(name=\"low\", description=\"Low priority\", priority=TaskPriority.LOW)\n        \n        assert high_task.priority.value > medium_task.priority.value\n        assert medium_task.priority.value > low_task.priority.value\n\n\nclass TestWorkflowConfig:\n    \"\"\"Test WorkflowConfig model.\"\"\"\n    \n    def test_workflow_config_creation(self):\n        \"\"\"Test creating workflow configuration.\"\"\"\n        tasks = [\n            TaskDefinition(name=\"task1\", description=\"First task\"),\n            TaskDefinition(name=\"task2\", description=\"Second task\", dependencies=[\"task1\"]),\n            TaskDefinition(name=\"task3\", description=\"Third task\", dependencies=[\"task2\"])\n        ]\n        \n        config = WorkflowConfig(\n            name=\"blog_review\",\n            description=\"Complete blog review workflow\",\n            tasks=tasks,\n            max_concurrent_tasks=2,\n            overall_timeout_seconds=1800\n        )\n        \n        assert config.name == \"blog_review\"\n        assert len(config.tasks) == 3\n        assert config.max_concurrent_tasks == 2\n        assert config.overall_timeout_seconds == 1800\n    \n    def test_workflow_config_validation(self):\n        \"\"\"Test workflow configuration validation.\"\"\"\n        # Test circular dependency detection\n        tasks = [\n            TaskDefinition(name=\"task1\", description=\"First task\", dependencies=[\"task2\"]),\n            TaskDefinition(name=\"task2\", description=\"Second task\", dependencies=[\"task1\"])\n        ]\n        \n        with pytest.raises(ValidationError, match=\"Circular dependency detected\"):\n            WorkflowConfig(\n                name=\"invalid_workflow\",\n                description=\"Workflow with circular dependencies\",\n                tasks=tasks\n            )\n    \n    def test_workflow_config_dependency_validation(self):\n        \"\"\"Test dependency validation.\"\"\"\n        tasks = [\n            TaskDefinition(name=\"task1\", description=\"First task\", dependencies=[\"nonexistent\"])\n        ]\n        \n        with pytest.raises(ValidationError, match=\"Unknown dependency\"):\n            WorkflowConfig(\n                name=\"invalid_workflow\",\n                description=\"Workflow with invalid dependency\",\n                tasks=tasks\n            )\n\n\nclass TestWorkflowMetrics:\n    \"\"\"Test WorkflowMetrics model.\"\"\"\n    \n    def test_workflow_metrics_creation(self):\n        \"\"\"Test creating workflow metrics.\"\"\"\n        start_time = datetime.utcnow()\n        end_time = start_time + timedelta(minutes=5)\n        \n        metrics = WorkflowMetrics(\n            workflow_id=ObjectId(),\n            start_time=start_time,\n            end_time=end_time,\n            total_tasks=5,\n            completed_tasks=5,\n            failed_tasks=0,\n            retried_tasks=1\n        )\n        \n        assert metrics.duration_seconds == 300  # 5 minutes\n        assert metrics.success_rate == 1.0\n        assert metrics.retry_rate == 0.2  # 1/5\n    \n    def test_workflow_metrics_calculations(self):\n        \"\"\"Test metrics calculations.\"\"\"\n        metrics = WorkflowMetrics(\n            workflow_id=ObjectId(),\n            start_time=datetime.utcnow(),\n            total_tasks=10,\n            completed_tasks=8,\n            failed_tasks=2,\n            retried_tasks=3\n        )\n        \n        assert metrics.success_rate == 0.8\n        assert metrics.failure_rate == 0.2\n        assert metrics.retry_rate == 0.3\n```\n\n```python\n# File: tests/unit/review_orchestrator/test_state_manager.py\n\"\"\"\nUnit tests for workflow state manager.\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import AsyncMock, Mock\nfrom datetime import datetime\nfrom bson import ObjectId\n\nfrom core.review_orchestrator.state_manager import WorkflowStateManager\nfrom core.review_orchestrator.models import WorkflowState, WorkflowStatus\n\n\nclass TestWorkflowStateManager:\n    \"\"\"Test WorkflowStateManager class.\"\"\"\n    \n    @pytest.fixture\n    def mock_db(self):\n        \"\"\"Create mock database.\"\"\"\n        db = AsyncMock()\n        collection = AsyncMock()\n        db.workflow_states = collection\n        return db\n    \n    @pytest.fixture\n    def state_manager(self, mock_db):\n        \"\"\"Create WorkflowStateManager instance.\"\"\"\n        return WorkflowStateManager(mock_db)\n    \n    @pytest.mark.asyncio\n    async def test_create_workflow_state(self, state_manager, mock_db):\n        \"\"\"Test creating a new workflow state.\"\"\"\n        workflow_id = ObjectId()\n        review_id = ObjectId()\n        \n        # Mock successful insert\n        mock_db.workflow_states.insert_one.return_value = AsyncMock()\n        mock_db.workflow_states.insert_one.return_value.inserted_id = workflow_id\n        \n        state = await state_manager.create_workflow_state(\n            workflow_id=workflow_id,\n            review_id=review_id,\n            total_steps=4\n        )\n        \n        assert state.workflow_id == workflow_id\n        assert state.review_id == review_id\n        assert state.status == WorkflowStatus.PENDING\n        assert state.total_steps == 4\n        \n        # Verify database call\n        mock_db.workflow_states.insert_one.assert_called_once()\n    \n    @pytest.mark.asyncio\n    async def test_get_workflow_state(self, state_manager, mock_db):\n        \"\"\"Test retrieving workflow state.\"\"\"\n        workflow_id = ObjectId()\n        \n        # Mock database response\n        mock_state_doc = {\n            \"_id\": ObjectId(),\n            \"workflow_id\": workflow_id,\n            \"review_id\": ObjectId(),\n            \"status\": \"pending\",\n            \"current_step\": \"init\",\n            \"total_steps\": 4,\n            \"completed_steps\": 0,\n            \"progress\": 0.0,\n            \"retry_count\": 0,\n            \"created_at\": datetime.utcnow(),\n            \"updated_at\": datetime.utcnow()\n        }\n        \n        mock_db.workflow_states.find_one.return_value = mock_state_doc\n        \n        state = await state_manager.get_workflow_state(workflow_id)\n        \n        assert state is not None\n        assert state.workflow_id == workflow_id\n        assert state.status == WorkflowStatus.PENDING\n        \n        # Verify database call\n        mock_db.workflow_states.find_one.assert_called_once_with(\n            {\"workflow_id\": workflow_id}\n        )\n    \n    @pytest.mark.asyncio\n    async def test_update_workflow_state(self, state_manager, mock_db):\n        \"\"\"Test updating workflow state.\"\"\"\n        workflow_id = ObjectId()\n        \n        # Mock successful update\n        mock_result = AsyncMock()\n        mock_result.modified_count = 1\n        mock_db.workflow_states.update_one.return_value = mock_result\n        \n        result = await state_manager.update_workflow_state(\n            workflow_id=workflow_id,\n            status=WorkflowStatus.IN_PROGRESS,\n            current_step=\"content_analysis\",\n            completed_steps=1\n        )\n        \n        assert result is True\n        \n        # Verify database call\n        mock_db.workflow_states.update_one.assert_called_once()\n        call_args = mock_db.workflow_states.update_one.call_args\n        assert call_args[0][0] == {\"workflow_id\": workflow_id}\n        assert \"status\" in call_args[0][1][\"$set\"]\n        assert \"current_step\" in call_args[0][1][\"$set\"]\n        assert \"updated_at\" in call_args[0][1][\"$set\"]\n    \n    @pytest.mark.asyncio\n    async def test_transition_workflow_state(self, state_manager, mock_db):\n        \"\"\"Test workflow state transitions.\"\"\"\n        workflow_id = ObjectId()\n        \n        # Mock current state\n        current_state = WorkflowState(\n            workflow_id=workflow_id,\n            review_id=ObjectId(),\n            status=WorkflowStatus.PENDING,\n            current_step=\"init\",\n            total_steps=4\n        )\n        \n        # Mock successful transition\n        mock_result = AsyncMock()\n        mock_result.modified_count = 1\n        mock_db.workflow_states.find_one.return_value = current_state.model_dump()\n        mock_db.workflow_states.update_one.return_value = mock_result\n        \n        result = await state_manager.transition_workflow_state(\n            workflow_id=workflow_id,\n            new_status=WorkflowStatus.IN_PROGRESS\n        )\n        \n        assert result is True\n    \n    @pytest.mark.asyncio\n    async def test_invalid_state_transition(self, state_manager, mock_db):\n        \"\"\"Test invalid state transition.\"\"\"\n        workflow_id = ObjectId()\n        \n        # Mock current state\n        current_state = WorkflowState(\n            workflow_id=workflow_id,\n            review_id=ObjectId(),\n            status=WorkflowStatus.COMPLETED,  # Cannot transition from completed to pending\n            current_step=\"done\",\n            total_steps=4\n        )\n        \n        mock_db.workflow_states.find_one.return_value = current_state.model_dump()\n        \n        with pytest.raises(ValueError, match=\"Invalid state transition\"):\n            await state_manager.transition_workflow_state(\n                workflow_id=workflow_id,\n                new_status=WorkflowStatus.PENDING\n            )\n    \n    @pytest.mark.asyncio\n    async def test_increment_retry_count(self, state_manager, mock_db):\n        \"\"\"Test incrementing retry count.\"\"\"\n        workflow_id = ObjectId()\n        \n        # Mock successful update\n        mock_result = AsyncMock()\n        mock_result.modified_count = 1\n        mock_db.workflow_states.update_one.return_value = mock_result\n        \n        result = await state_manager.increment_retry_count(workflow_id)\n        \n        assert result is True\n        \n        # Verify database call\n        mock_db.workflow_states.update_one.assert_called_once()\n        call_args = mock_db.workflow_states.update_one.call_args\n        assert call_args[0][0] == {\"workflow_id\": workflow_id}\n        assert \"$inc\" in call_args[0][1]\n        assert call_args[0][1][\"$inc\"][\"retry_count\"] == 1\n```\n\n```python\n# File: tests/unit/review_orchestrator/test_task_queue.py\n\"\"\"\nUnit tests for task queue system.\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom datetime import datetime, timedelta\nfrom unittest.mock import AsyncMock, Mock\nfrom bson import ObjectId\n\nfrom core.review_orchestrator.task_queue import (\n    TaskQueue, TaskItem, TaskStatus, QueuedTask\n)\nfrom core.review_orchestrator.models import TaskDefinition, TaskPriority\n\n\nclass TestTaskItem:\n    \"\"\"Test TaskItem model.\"\"\"\n    \n    def test_task_item_creation(self):\n        \"\"\"Test creating a task item.\"\"\"\n        task_def = TaskDefinition(\n            name=\"test_task\",\n            description=\"Test task\",\n            priority=TaskPriority.HIGH\n        )\n        \n        item = TaskItem(\n            task_id=ObjectId(),\n            workflow_id=ObjectId(),\n            task_definition=task_def,\n            context={\"test\": \"data\"}\n        )\n        \n        assert item.status == TaskStatus.PENDING\n        assert item.priority == TaskPriority.HIGH\n        assert item.retry_count == 0\n        assert isinstance(item.created_at, datetime)\n    \n    def test_task_item_priority_comparison(self):\n        \"\"\"Test task item priority comparison.\"\"\"\n        high_task = TaskItem(\n            task_id=ObjectId(),\n            workflow_id=ObjectId(),\n            task_definition=TaskDefinition(\n                name=\"high\", description=\"High priority\", priority=TaskPriority.HIGH\n            )\n        )\n        \n        low_task = TaskItem(\n            task_id=ObjectId(),\n            workflow_id=ObjectId(),\n            task_definition=TaskDefinition(\n                name=\"low\", description=\"Low priority\", priority=TaskPriority.LOW\n            )\n        )\n        \n        # High priority tasks should be processed first\n        assert high_task < low_task  # For priority queue ordering\n\n\nclass TestTaskQueue:\n    \"\"\"Test TaskQueue class.\"\"\"\n    \n    @pytest.fixture\n    def task_queue(self):\n        \"\"\"Create TaskQueue instance.\"\"\"\n        return TaskQueue(max_concurrent_tasks=3)\n    \n    def test_task_queue_creation(self, task_queue):\n        \"\"\"Test creating task queue.\"\"\"\n        assert task_queue.max_concurrent_tasks == 3\n        assert task_queue.active_tasks == 0\n        assert task_queue.is_running is False\n    \n    @pytest.mark.asyncio\n    async def test_add_task(self, task_queue):\n        \"\"\"Test adding task to queue.\"\"\"\n        task_def = TaskDefinition(\n            name=\"test_task\",\n            description=\"Test task\",\n            priority=TaskPriority.HIGH\n        )\n        \n        task_id = await task_queue.add_task(\n            workflow_id=ObjectId(),\n            task_definition=task_def,\n            context={\"test\": \"data\"}\n        )\n        \n        assert task_id is not None\n        assert task_queue.queue_size() == 1\n    \n    @pytest.mark.asyncio\n    async def test_priority_ordering(self, task_queue):\n        \"\"\"Test that tasks are processed by priority.\"\"\"\n        # Add tasks in reverse priority order\n        low_task_def = TaskDefinition(\n            name=\"low_task\", description=\"Low priority\", priority=TaskPriority.LOW\n        )\n        high_task_def = TaskDefinition(\n            name=\"high_task\", description=\"High priority\", priority=TaskPriority.HIGH\n        )\n        medium_task_def = TaskDefinition(\n            name=\"medium_task\", description=\"Medium priority\", priority=TaskPriority.MEDIUM\n        )\n        \n        low_task_id = await task_queue.add_task(ObjectId(), low_task_def)\n        high_task_id = await task_queue.add_task(ObjectId(), high_task_def)\n        medium_task_id = await task_queue.add_task(ObjectId(), medium_task_def)\n        \n        # Mock task execution to just return success\n        async def mock_execute(task_item):\n            return {\"status\": \"completed\", \"result\": \"success\"}\n        \n        task_queue._execute_task = mock_execute\n        \n        # Start processing\n        await task_queue.start()\n        \n        # Let tasks process\n        await asyncio.sleep(0.1)\n        \n        # High priority task should be processed first\n        # This test verifies priority ordering indirectly\n        assert task_queue.queue_size() <= 3  # All tasks should be processed or processing\n        \n        await task_queue.stop()\n    \n    @pytest.mark.asyncio\n    async def test_concurrent_task_limit(self, task_queue):\n        \"\"\"Test concurrent task execution limit.\"\"\"\n        # Create slow tasks that will help us test concurrency\n        async def slow_task_executor(task_item):\n            await asyncio.sleep(0.2)  # Simulate work\n            return {\"status\": \"completed\"}\n        \n        task_queue._execute_task = slow_task_executor\n        \n        # Add more tasks than the concurrent limit\n        task_def = TaskDefinition(name=\"test\", description=\"Test task\")\n        for i in range(5):\n            await task_queue.add_task(ObjectId(), task_def)\n        \n        await task_queue.start()\n        \n        # Let some tasks start\n        await asyncio.sleep(0.05)\n        \n        # Should not exceed max concurrent tasks\n        assert task_queue.active_tasks <= task_queue.max_concurrent_tasks\n        \n        await task_queue.stop()\n    \n    @pytest.mark.asyncio\n    async def test_task_retry_mechanism(self, task_queue):\n        \"\"\"Test task retry on failure.\"\"\"\n        retry_count = 0\n        \n        async def failing_task_executor(task_item):\n            nonlocal retry_count\n            retry_count += 1\n            if retry_count < 3:\n                raise Exception(\"Simulated failure\")\n            return {\"status\": \"completed\", \"result\": \"success\"}\n        \n        task_queue._execute_task = failing_task_executor\n        \n        task_def = TaskDefinition(\n            name=\"failing_task\",\n            description=\"Task that fails initially\",\n            retry_policy={\"max_retries\": 3, \"base_delay_seconds\": 0.01}\n        )\n        \n        task_id = await task_queue.add_task(ObjectId(), task_def)\n        \n        await task_queue.start()\n        await asyncio.sleep(0.5)  # Allow retries\n        await task_queue.stop()\n        \n        # Task should eventually succeed after retries\n        assert retry_count == 3\n    \n    @pytest.mark.asyncio\n    async def test_task_timeout(self, task_queue):\n        \"\"\"Test task timeout handling.\"\"\"\n        async def slow_task_executor(task_item):\n            await asyncio.sleep(1.0)  # Longer than timeout\n            return {\"status\": \"completed\"}\n        \n        task_queue._execute_task = slow_task_executor\n        \n        task_def = TaskDefinition(\n            name=\"slow_task\",\n            description=\"Task that times out\",\n            timeout_seconds=0.1  # Very short timeout\n        )\n        \n        task_id = await task_queue.add_task(ObjectId(), task_def)\n        \n        await task_queue.start()\n        await asyncio.sleep(0.3)  # Let timeout occur\n        await task_queue.stop()\n        \n        # Task should have been cancelled due to timeout\n        task_status = await task_queue.get_task_status(task_id)\n        assert task_status[\"status\"] in [TaskStatus.FAILED, TaskStatus.CANCELLED]\n    \n    @pytest.mark.asyncio\n    async def test_get_task_status(self, task_queue):\n        \"\"\"Test getting task status.\"\"\"\n        task_def = TaskDefinition(name=\"test\", description=\"Test\")\n        task_id = await task_queue.add_task(ObjectId(), task_def)\n        \n        status = await task_queue.get_task_status(task_id)\n        assert status[\"status\"] == TaskStatus.PENDING\n        assert status[\"retry_count\"] == 0\n    \n    @pytest.mark.asyncio\n    async def test_cancel_task(self, task_queue):\n        \"\"\"Test cancelling a task.\"\"\"\n        task_def = TaskDefinition(name=\"test\", description=\"Test\")\n        task_id = await task_queue.add_task(ObjectId(), task_def)\n        \n        result = await task_queue.cancel_task(task_id)\n        assert result is True\n        \n        status = await task_queue.get_task_status(task_id)\n        assert status[\"status\"] == TaskStatus.CANCELLED\n    \n    @pytest.mark.asyncio\n    async def test_queue_metrics(self, task_queue):\n        \"\"\"Test queue metrics collection.\"\"\"\n        # Add some tasks\n        task_def = TaskDefinition(name=\"test\", description=\"Test\")\n        for i in range(3):\n            await task_queue.add_task(ObjectId(), task_def)\n        \n        metrics = task_queue.get_metrics()\n        \n        assert metrics[\"queue_size\"] == 3\n        assert metrics[\"active_tasks\"] == 0\n        assert metrics[\"total_tasks\"] == 3\n        assert \"average_wait_time\" in metrics\n        assert \"throughput\" in metrics\n```\n\n```python\n# File: tests/unit/review_orchestrator/test_orchestrator.py\n\"\"\"\nUnit tests for workflow orchestrator.\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import AsyncMock, Mock, patch\nfrom datetime import datetime\nfrom bson import ObjectId\n\nfrom core.review_orchestrator.orchestrator import WorkflowOrchestrator\nfrom core.review_orchestrator.models import (\n    WorkflowConfig, WorkflowStatus, TaskDefinition, TaskPriority\n)\n\n\nclass TestWorkflowOrchestrator:\n    \"\"\"Test WorkflowOrchestrator class.\"\"\"\n    \n    @pytest.fixture\n    def mock_dependencies(self):\n        \"\"\"Create mock dependencies.\"\"\"\n        return {\n            \"db\": AsyncMock(),\n            \"task_queue\": AsyncMock(),\n            \"state_manager\": AsyncMock(),\n            \"monitor\": AsyncMock(),\n            \"error_handler\": AsyncMock()\n        }\n    \n    @pytest.fixture\n    def orchestrator(self, mock_dependencies):\n        \"\"\"Create WorkflowOrchestrator instance.\"\"\"\n        return WorkflowOrchestrator(\n            db=mock_dependencies[\"db\"],\n            task_queue=mock_dependencies[\"task_queue\"],\n            state_manager=mock_dependencies[\"state_manager\"],\n            monitor=mock_dependencies[\"monitor\"],\n            error_handler=mock_dependencies[\"error_handler\"]\n        )\n    \n    @pytest.fixture\n    def sample_workflow_config(self):\n        \"\"\"Create sample workflow configuration.\"\"\"\n        return WorkflowConfig(\n            name=\"blog_review\",\n            description=\"Blog review workflow\",\n            tasks=[\n                TaskDefinition(\n                    name=\"content_analysis\",\n                    description=\"Analyze content\",\n                    priority=TaskPriority.HIGH\n                ),\n                TaskDefinition(\n                    name=\"style_review\",\n                    description=\"Review writing style\",\n                    priority=TaskPriority.MEDIUM,\n                    dependencies=[\"content_analysis\"]\n                ),\n                TaskDefinition(\n                    name=\"grammar_check\",\n                    description=\"Check grammar\",\n                    priority=TaskPriority.MEDIUM,\n                    dependencies=[\"content_analysis\"]\n                ),\n                TaskDefinition(\n                    name=\"final_report\",\n                    description=\"Generate final report\",\n                    priority=TaskPriority.HIGH,\n                    dependencies=[\"style_review\", \"grammar_check\"]\n                )\n            ]\n        )\n    \n    @pytest.mark.asyncio\n    async def test_start_workflow(self, orchestrator, sample_workflow_config, mock_dependencies):\n        \"\"\"Test starting a new\n\n```python\nasync def test_start_workflow(self, orchestrator, sample_workflow_config, mock_dependencies):\n        \"\"\"Test starting a new workflow.\"\"\"\n        # Mock database response\n        mock_dependencies[\"db\"].create_workflow.return_value = \"workflow-123\"\n        \n        # Start workflow\n        workflow_id = await orchestrator.start_workflow(\n            config=sample_workflow_config,\n            context={\"pr_id\": \"pr-456\", \"repo\": \"test/repo\"}\n        )\n        \n        # Verify workflow creation\n        assert workflow_id == \"workflow-123\"\n        mock_dependencies[\"db\"].create_workflow.assert_called_once()\n        mock_dependencies[\"state_manager\"].initialize_workflow.assert_called_once_with(\n            workflow_id, sample_workflow_config\n        )\n    \n    @pytest.mark.asyncio\n    async def test_execute_workflow_success(self, orchestrator, mock_dependencies):\n        \"\"\"Test successful workflow execution.\"\"\"\n        workflow_id = \"workflow-123\"\n        \n        # Mock initial ready tasks\n        mock_dependencies[\"state_manager\"].get_ready_tasks.side_effect = [\n            [\"content_analysis\"],  # First call\n            [\"style_review\", \"grammar_check\"],  # After content_analysis completes\n            [\"final_report\"],  # After style_review and grammar_check complete\n            []  # No more tasks\n        ]\n        \n        # Mock task completion\n        mock_dependencies[\"task_queue\"].execute_task.return_value = TaskResult(\n            task_id=\"task-1\",\n            status=TaskStatus.COMPLETED,\n            result={\"analysis\": \"complete\"}\n        )\n        \n        # Execute workflow\n        result = await orchestrator.execute_workflow(workflow_id)\n        \n        # Verify execution\n        assert result.status == WorkflowStatus.COMPLETED\n        assert mock_dependencies[\"task_queue\"].execute_task.call_count == 4\n        mock_dependencies[\"monitor\"].log_workflow_completion.assert_called_once()\n    \n    @pytest.mark.asyncio\n    async def test_execute_workflow_with_failure(self, orchestrator, mock_dependencies):\n        \"\"\"Test workflow execution with task failure.\"\"\"\n        workflow_id = \"workflow-123\"\n        \n        # Mock task failure\n        mock_dependencies[\"state_manager\"].get_ready_tasks.return_value = [\"content_analysis\"]\n        mock_dependencies[\"task_queue\"].execute_task.return_value = TaskResult(\n            task_id=\"content_analysis\",\n            status=TaskStatus.FAILED,\n            error=\"Analysis failed\"\n        )\n        \n        # Execute workflow\n        result = await orchestrator.execute_workflow(workflow_id)\n        \n        # Verify failure handling\n        assert result.status == WorkflowStatus.FAILED\n        mock_dependencies[\"error_handler\"].handle_workflow_failure.assert_called_once()\n    \n    @pytest.mark.asyncio\n    async def test_pause_workflow(self, orchestrator, mock_dependencies):\n        \"\"\"Test pausing a workflow.\"\"\"\n        workflow_id = \"workflow-123\"\n        \n        # Pause workflow\n        await orchestrator.pause_workflow(workflow_id)\n        \n        # Verify pause\n        mock_dependencies[\"state_manager\"].pause_workflow.assert_called_once_with(workflow_id)\n        mock_dependencies[\"task_queue\"].cancel_pending_tasks.assert_called_once_with(workflow_id)\n    \n    @pytest.mark.asyncio\n    async def test_resume_workflow(self, orchestrator, mock_dependencies):\n        \"\"\"Test resuming a paused workflow.\"\"\"\n        workflow_id = \"workflow-123\"\n        \n        # Mock ready tasks after resume\n        mock_dependencies[\"state_manager\"].get_ready_tasks.return_value = [\"style_review\"]\n        \n        # Resume workflow\n        await orchestrator.resume_workflow(workflow_id)\n        \n        # Verify resume\n        mock_dependencies[\"state_manager\"].resume_workflow.assert_called_once_with(workflow_id)\n\n\nif __name__ == \"__main__\":\n    import asyncio\n    \n    # Example usage\n    async def main():\n        # Initialize orchestrator\n        orchestrator = WorkflowOrchestrator()\n        \n        # Create workflow configuration\n        config = WorkflowConfig(\n            name=\"pr_review\",\n            description=\"Pull Request Review Workflow\",\n            tasks=[\n                TaskDefinition(\n                    name=\"security_scan\",\n                    description=\"Security vulnerability scan\",\n                    priority=TaskPriority.HIGH\n                ),\n                TaskDefinition(\n                    name=\"code_quality\",\n                    description=\"Code quality analysis\",\n                    priority=TaskPriority.HIGH\n                ),\n                TaskDefinition(\n                    name=\"test_coverage\",\n                    description=\"Test coverage analysis\",\n                    priority=TaskPriority.MEDIUM,\n                    dependencies=[\"code_quality\"]\n                ),\n                TaskDefinition(\n                    name=\"performance_review\",\n                    description=\"Performance impact analysis\",\n                    priority=TaskPriority.MEDIUM\n                ),\n                TaskDefinition(\n                    name=\"final_review\",\n                    description=\"Generate comprehensive review\",\n                    priority=TaskPriority.HIGH,\n                    dependencies=[\"security_scan\", \"test_coverage\", \"performance_review\"]\n                )\n            ]\n        )\n        \n        # Start and execute workflow\n        try:\n            workflow_id = await orchestrator.start_workflow(\n                config=config,\n                context={\n                    \"pr_id\": \"123\",\n                    \"repo\": \"example/repo\",\n                    \"branch\": \"feature/new-feature\"\n                }\n            )\n            \n            print(f\"Started workflow: {workflow_id}\")\n            \n            # Execute workflow\n            result = await orchestrator.execute_workflow(workflow_id)\n            print(f\"Workflow completed with status: {result.status}\")\n            \n            if result.status == WorkflowStatus.COMPLETED:\n                print(\"Review results:\")\n                for task_result in result.task_results:\n                    print(f\"- {task_result.task_id}: {task_result.status}\")\n            \n        except Exception as e:\n            print(f\"Workflow execution failed: {e}\")\n    \n    asyncio.run(main())\n```\n\n## Summary\n\nThe Workflow Orchestrator implementation provides:\n\n### Core Features\n- **Workflow Definition**: Flexible task configuration with dependencies and priorities\n- **Execution Engine**: Async task scheduling and parallel execution\n- **State Management**: Workflow and task state tracking with persistence\n- **Error Handling**: Comprehensive error recovery and failure management\n- **Monitoring**: Real-time workflow progress and performance metrics\n\n### Key Components\n1. **WorkflowOrchestrator**: Main orchestration class handling workflow lifecycle\n2. **TaskScheduler**: Intelligent task scheduling with dependency resolution\n3. **WorkflowState**: State management with database persistence\n4. **WorkflowMonitor**: Performance monitoring and metrics collection\n5. **ErrorHandler**: Robust error handling and recovery mechanisms\n\n### Usage Patterns\n- **PR Reviews**: Coordinate multiple analysis tasks (security, quality, performance)\n- **Content Processing**: Orchestrate content analysis workflows\n- **Data Pipelines**: Manage complex data processing workflows\n- **Testing Workflows**: Coordinate test execution and reporting\n\n### Benefits\n- **Scalable**: Handles complex workflows with many dependencies\n- **Reliable**: Comprehensive error handling and recovery\n- **Flexible**: Configurable task definitions and execution strategies\n- **Observable**: Rich monitoring and logging capabilities\n- **Maintainable**: Clean separation of concerns and testable components\n\nThe orchestrator integrates seamlessly with existing systems while providing the flexibility to handle various workflow patterns and requirements.",
  "attempt": 2,
  "status": "partial"
}